<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>CS180 Project 2: Fun with Filters and Frequencies</title>
  <style>
    /* Minimal CSS as requested */
    body { font-family: Arial, Helvetica, sans-serif; margin: 20px; color: #111; line-height: 1.5; }
    h1 { margin-bottom: 8px; }
    h2 { margin-top: 28px; margin-bottom: 6px; }
    h3 { margin-top: 18px; margin-bottom: 6px; }
    .section { margin-bottom: 28px; }
    .images { margin: 6px 0 12px 0; }
    .images img { max-width: 100%; width: 800px; margin: 12px 0; border: 1px solid #ccc; display: block; }
    p { margin: 10px 0; }
    pre { background: #f5f5f5; padding: 8px; overflow: auto; }
    .note { color: #555; font-size: 0.95em; }
    .missing { color: #a00; font-weight: bold; }
  </style>
</head>
<body>
  <h1>CS180 Project 2: Fun with Filters and Frequencies</h1>

  <!-- PART 1 -->
  <div class="section">
    <h2>Part 1: Filters and Edges</h2>

    <h3>1.1 Convolution</h3>
    <div class="images">
      <img src="mediatwo/1.1-box.png" alt="1.1 box">
      <img src="mediatwo/1.1-dx-dy.png" alt="1.1 dx dy">
    </div>
    <p>
      The runtime is nearly identical for greyscale images between my convolution and scipy's .convolve2d, although mine is slightly slower with color images. To deal with padding boundaries I initially cropped but realized I was losing information, so I ended up reflecting the boundaries like scipy did. I implemented my convolution with four for loops, and then two, saving time by multiplying patches of the image by the kernel instead of going element by element. I implemented padding with zeroes. I ran my two loop convolution against scipy's convolution on an image of me in grayscale using: a box filter, a Dx filter, and a Dy filter. Here is my code for this section
    </p>

    <pre><code class="language-python">
def four_for_convolution(input_image, filter_type="box_filter", kernel_size=9):
    if filter_type == "box_filter":
        kernel_values = np.full((kernel_size, kernel_size), 1/(kernel_size*kernel_size))
        kernel_array = np.flip(kernel_values)
        pad_h = kernel_size // 2
        pad_w = kernel_size // 2
    if filter_type == "dx":
        kernel_values = np.array([[-1, 0, 1]])
        kernel_array = np.flip(kernel_values)
        pad_h = 1
        pad_w = 0
    if filter_type == "dy":
        kernel_values = np.array([[-1], [0], [1]])
        kernel_array = np.flip(kernel_values)
        pad_h = 0
        pad_w = 1
    input_width = input_image.shape[1]
    input_height = input_image.shape[0]
    output_image = np.empty((input_height, input_width))

    padded_image = np.pad(input_image, ((pad_h, pad_h), (pad_w, pad_w)), mode='constant', constant_values=0)

    for y in np.arange(input_height):
        for x in np.arange(input_width):
            sum = 0
            for j in np.arange(kernel_array.shape[1]):
                for i in np.arange(kernel_array.shape[0]):
                    hy = y + j
                    hx = x + i
                    sum += kernel_array[j, i] * padded_image[hy, hx]
            output_image[y,x] = sum
    return output_image
    </code></pre>

    <h3>1.2 Partial Derivatives and Edges</h3>
    <div class="images">
      <img src="mediatwo/1.2-dx-dy.png" alt="1.2 dx dy">
      <img src="mediatwo/1.2-gm.png" alt="1.2 gradient magnitude">
      <img src="mediatwo/1.2-bgm.png" alt="1.2 binarized gradient magnitude">
      <img src="mediatwo/1.2-g-gm-bgm.png" alt="1.2 combined visuals">
    </div>
    <p>
      I used the scipy.convolve2d function for all further parts. I calculated the partial derivatives in x and y for the cameraman image vy convolving the image with finite difference operators D_x and D_y. I calculated a gradient magnitude image by taking the Dx image and Dy image and calculating the sqrt of square(Dx) + square(Dy). This only displays the edges of the images, both images on the x plane and the y plane. However, without binarizing the edges, the gradient magnitude image looks very noisy To fix this, we suppress the noise while retaining edges by setting all pixels to either a 0 (black) or 1 (white) depending on if the pixel's grayscale value is larger than a certain threshold. For my threshold I choose 60 as it retained most of the cameraman's detail while reducing the visible noise on the ground. Although I did find threshold 20 to be more aesthetically pleasing with the snow-like noise texture, even if it did have too much detail for the cameraman.
    </p>

    <pre><code>
def gradient_magnitude_image(input_image):
    dx = scipy_convolution(input_image, filter_type="dx")
    dy = scipy_convolution(input_image, filter_type="dy")
    return np.sqrt(np.square(dx) + np.square(dy))

def binarize_gradient_magnitude(img, threshold=60):
    img = (img * 255).astype(np.uint8)
    return (img > threshold)
    </code></pre>

    <h3>1.3 Gaussian & DoG Filters</h3>
    <div class="images">
      <img src="mediatwo/1.3-dog-filters.png" alt="1.3 dog filters">
      <img src="mediatwo/1.3-DoG-gm-bgm.png" alt="1.3 DoG gm bgm">
    </div>
    <p>
     Even so, with just the difference operators Dx and Dy, the image contained a lot of noise that even binarizing the image couldn't fix. To fix this I blurred the image using a gaussian kernel, and then repeated the procedure in the previous part. This got rid of almost all noise and made edges thicker and seemingly brighter. To make this faster I implemented DoG or derivate of gaussian filters which looks the same but convolves the gaussian kernel with D_x and D_y before convolving with the image, which saves one convolution in total.
    </p>

  <!-- PART 2 -->
  <div class="section">
    <h2>Part 2: Applications</h2>

    <h3>2.1 Unsharp Mask</h3>
    <div class="images">
      <img src="mediatwo/2.1-sharpen-taj.png" alt="2.1 sharpen taj">
      <img src="mediatwo/2.1-sharpen-mine.png" alt="2.1 sharpen mine">
    </div>
    <p>
      I took a blurry image and sharpened it by subtracting low filters from my image to retain only the high frequencies. This makes the image appear sharper because it has stronger high frequencies. I did so by running a gaussian filter on my image, which is a low pass filter. This means it only retains low frequencies. Subtracting this gaussian filtered image from the original image retains only the high frequencies / edges. I then added these edges back to the image to sharpen it. I also tried this on a image that was sharpened, blurred, and then sharpened again. However, this does not look the same it only retained the high frequencies of the blurred image not the high frequencies of the original image, since those were lost when I low pass / gaussian filter it and removed high frequencies

    <pre><code>
def sharpen(input_image, alpha=1.0):
    blurred = scipy_convolution_color(input_image, "gaussian", 9, 5)
    mask = input_image - blurred
    return np.clip(input_image + alpha * mask, 0, 255)
    </code></pre>

    <h3>2.2 Hybrid Images</h3>
    <div class="images">
      <img src="mediatwo/2.2-d-and-n.png" alt="2.2 Derek and Nutmeg">
      <img src="mediatwo/2.2-low-high-hyb.png" alt="2.2 low/high hybrid">
      <img src="mediatwo/2.2-my-lhh.png" alt="2.2 my low/high hybrid">
      <img src="mediatwo/2.2.png" alt="2.2 other hybrid">
      <img src="mediatwo/2.2-fourrier.png" alt="2.2 fourier plots">
    </div>
    <p>
      I created a hybrid image (a static image that changes in interpretation as a function of viewing distance) by overlaying a high pass of one image (just the high frequencies) with a low pass of another image (just the low frequencies). From up close the high frequencies dominate, so we mainly see the high frequency image but at a distance the low frequencies are all we can see. I used some alignment code to align all my images. I began with the images of Derek and his cat Nutmeg. I then did this with two of my own images, one of me and my cat, and another of my cat and lebron james. I then plotted the fourier transform of my low pass and high pass images compared to my original images to show which frequencies were being filtered out.
    </p>

    <h3>2.3 + 2.4 Image Blending (Oraple)</h3>
    <div class="images">
      <img src="mediatwo/2.3-stack1.png" alt="2.3 stack1">
      <img src="mediatwo/2.3-stack2.png" alt="2.3 stack2">
      <img src="mediatwo/2.3-stack3.png" alt="2.3 stack3">
    </div>
    <div class="images">
      <img src="mediatwo/2.3-oraple.png" alt="2.3 oraple">
      <img src="mediatwo/2.3-ryan-emir.png" alt="2.3 ryanemir">
    </div>
    <p>
      I created Gaussian and Laplacian stacks to blend together two images. A Gaussian stack is a series of images where each level in the stack is the previous layer's image with a gaussian filter applied, so it becomes blurrier and blurrier. A laplacian stack is similar in that each layer is gaussian layer i minus gaussian layer i+1 so it displayes edges but each sucessive image is sharpening a blurrier image. I applied these stacks to the Oraple on a vertical seam. I did the same for a image of myself and my friend in makeup. I then did this with my friend's eyes and her dog using an irregular mask, but both use the same process and logic as described below.
    </p>

  </div>
</body>
</html>










