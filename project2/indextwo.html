<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>CS180 Project 2: Fun with Filters and Frequencies</title>
  <style>
    /* Minimal CSS as requested */
    body { font-family: Arial, Helvetica, sans-serif; margin: 20px; color: #111; line-height: 1.5; }
    h1 { margin-bottom: 8px; }
    h2 { margin-top: 28px; margin-bottom: 6px; }
    h3 { margin-top: 18px; margin-bottom: 6px; }
    .section { margin-bottom: 28px; }
    .images { margin: 6px 0 12px 0; }
    .images img { max-width: 320px; margin: 6px; border: 1px solid #ccc; display: inline-block; vertical-align: top; }
    p { margin: 10px 0; }
    pre { background: #f5f5f5; padding: 8px; overflow: auto; }
    .note { color: #555; font-size: 0.95em; }
    .missing { color: #a00; font-weight: bold; }
  </style>
</head>
<body>
  <h1>CS180 Project 2: Fun with Filters and Frequencies</h1>

  <!-- PART 1 -->
  <div class="section">
    <h2>Part 1: Filters and Edges</h2>

    <h3>1.1 Convolution</h3>
    <div class="images">
      <img src="../mediatwo/1.1 box.png" alt="1.1 box">
      <img src="../mediatwo/1.1 dx dy.png" alt="1.1 dx dy">
    </div>
    <p>
      The runtime is nearly identical for greyscale images between my convolution and scipy's <code>convolve2d</code>,
      although mine is slightly slower with color images. To deal with padding boundaries I initially cropped but realized
      I was losing information, so I ended up reflecting the boundaries like scipy did. I implemented my convolution with four
      for-loops, and then a faster two-for-loop version which multiplies image patches by the kernel instead of iterating
      element-by-element. I implemented padding with zeroes for some experiments and with reflection for final comparisons.
    </p>

    <pre><code class="language-python">
def four_for_convolution(input_image, filter_type="box_filter", kernel_size=9):
    if filter_type == "box_filter":
        kernel_values = np.full((kernel_size, kernel_size), 1/(kernel_size*kernel_size))
        kernel_array = np.flip(kernel_values)
        pad_h = kernel_size // 2
        pad_w = kernel_size // 2
    if filter_type == "dx":
        kernel_values = np.array([[-1, 0, 1]])
        kernel_array = np.flip(kernel_values)
        pad_h = 1
        pad_w = 0
    if filter_type == "dy":
        kernel_values = np.array([[-1], [0], [1]])
        kernel_array = np.flip(kernel_values)
        pad_h = 0
        pad_w = 1
    input_width = input_image.shape[1]
    input_height = input_image.shape[0]
    output_image = np.empty((input_height, input_width))

    padded_image = np.pad(input_image, ((pad_h, pad_h), (pad_w, pad_w)), mode='constant', constant_values=0)

    for y in np.arange(input_height):
        for x in np.arange(input_width):
            sum = 0
            for j in np.arange(kernel_array.shape[1]):
                for i in np.arange(kernel_array.shape[0]):
                    hy = y + j
                    hx = x + i
                    sum += kernel_array[j, i] * padded_image[hy, hx]
            output_image[y,x] = sum
    return output_image
    </code></pre>

    <h3>1.2 Partial Derivatives and Edges</h3>
    <div class="images">
      <img src="../mediatwo/1.2 dx dy.png" alt="1.2 dx dy">
      <img src="../mediatwo/1.2 gm.png" alt="1.2 gradient magnitude">
      <img src="../mediatwo/1.2 bgm.png" alt="1.2 binarized gradient magnitude">
      <img src="../mediatwo/1.2 g gm bgm.png" alt="1.2 combined visuals">
    </div>
    <p>
      I used <code>scipy.signal.convolve2d</code> for the FD experiments. I computed partial derivatives Dx and Dy using
      finite difference kernels, then gradient magnitude via <code>sqrt(Dx^2 + Dy^2)</code>. The raw magnitude image is noisy,
      so I binarized using a threshold (I used 60/255 in one run) to retain main edges while suppressing background noise.
      I experimented with lower thresholds (e.g. 20) which preserved more fine detail but also amplified noise.
    </p>

    <pre><code>
def gradient_magnitude_image(input_image):
    dx = scipy_convolution(input_image, filter_type="dx")
    dy = scipy_convolution(input_image, filter_type="dy")
    return np.sqrt(np.square(dx) + np.square(dy))

def binarize_gradient_magnitude(img, threshold=60):
    img = (img * 255).astype(np.uint8)
    return (img > threshold)
    </code></pre>

    <h3>1.3 Gaussian & DoG Filters</h3>
    <div class="images">
      <img src="../mediatwo/1.3 dog filters.png" alt="1.3 dog filters">
      <img src="../mediatwo/1.3 DoG gm bgm.png" alt="1.3 DoG gm bgm">
    </div>
    <p>
      Using <code>cv2.getGaussianKernel</code> I built Gaussian kernels and formed DoG (difference of Gaussians). Applying
      Gaussian smoothing before computing gradients reduces noise and thickens edges. DoG filters that convolve the Gaussian
      with derivative filters produce similar responses to smoothing-then-differentiation but can be implemented more efficiently
      (precompute the combined kernel).
    </p>

    <h4>Bells & Whistles: Gradient Orientations</h4>
    <div class="images">
      <!-- If you have a gradient-orientation image, name it e.g. '1.2 orientation.png' and put it here -->
      <img src="../mediatwo/1.2 orientation.png" alt="gradient orientation (optional)">
    </div>
    <p class="note">
      You can compute orientation via <code>atan2(dy, dx)</code>. If you avoid built-in angle funcs, implement a safe atan2
      approximation or compute angle sectors using dy/dx sign and ratio logic â€” then map to hue for visualization.
    </p>
  </div>

  <!-- PART 2 -->
  <div class="section">
    <h2>Part 2: Applications</h2>

    <h3>2.1 Unsharp Mask</h3>
    <div class="images">
      <img src="../mediatwo/2.1 sharpen taj.png" alt="2.1 sharpen taj">
      <img src="../mediatwo/2.1 sharpen mine.png" alt="2.1 sharpen mine">
    </div>
    <p>
      I sharpened images by subtracting a Gaussian-blurred version (low-pass) from the original to obtain the high-frequency mask,
      then adding a scaled amount of that mask back to the original: <code>sharpened = original + amount * (original - blurred)</code>.
      Varying <em>amount</em> changes the strength of sharpening; very large values produce halos and clipping.
    </p>

    <pre><code>
def sharpen(input_image, alpha=1.0):
    blurred = scipy_convolution_color(input_image, "gaussian", 9, 5)
    mask = input_image - blurred
    return np.clip(input_image + alpha * mask, 0, 255)
    </code></pre>

    <h3>2.2 Hybrid Images</h3>
    <div class="images">
      <img src="../mediatwo/2.2 d and n.png" alt="2.2 Derek and Nutmeg">
      <img src="../mediatwo/2.2 low high hyb.png" alt="2.2 low/high hybrid">
      <img src="../mediatwo/2.2 my lhh.png" alt="2.2 my low/high hybrid">
      <img src="../mediatwo/2.2.png" alt="2.2 other hybrid">
      <img src="../mediatwo/2.2 fourrier.png" alt="2.2 fourier plots">
    </div>
    <p>
      Hybrid images combine the low frequencies of one picture with the high frequencies of another. The Fourier transforms show
      which bands are kept/removed. For one hybrid I aligned images (translation & scale), computed FFTs, applied radial low-pass
      and high-pass filters (cutoff chosen by visual inspection and frequency plots), inverse FFT, and combined results.
    </p>

    <h3>2.3 + 2.4 Image Blending (Oraple)</h3>
    <div class="images">
      <img src="../mediatwo/2.3 stack1.png" alt="2.3 stack1">
      <img src="../mediatwo/2.3 stack2.png" alt="2.3 stack2">
      <img src="../mediatwo/2.3 stack3.png" alt="2.3 stack3">
    </div>
    <div class="images">
      <img src="../mediatwo/2.3 oraple.png" alt="2.3 oraple">
      <img src="../mediatwo/2.3 ryanemir.png" alt="2.3 ryanemir">
    </div>
    <p>
      I constructed Gaussian and Laplacian stacks and performed multi-scale blending for the orange+apple example (vertical seam).
      Each Laplacian level preserves band-limited details; the mask's Gaussian pyramid smoothly transitions contributions across scales.
      I also blended a portrait (irregular mask) using the same pipeline and noted how mask smoothing and sigma choices affect seams and
      color mixing across levels.
    </p>

    <p class="note">
      <strong>Important:</strong> Put the exact image files in the same folder as this HTML and use the exact filenames used above (e.g.
      <code>1.1 box.png</code>, <code>1.2 gm.png</code>, <code>2.3 oraple.png</code>, etc.). If your files have different extensions
      (jpg/png) or slightly different names, either rename them to match these names or update the <code>src</code> attributes.
    </p>
  </div>
</body>
</html>

