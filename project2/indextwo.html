<!DOCTYPE html>
<style>
width: 100%;
height: 120px;
margin-top: 10px;
}
.image-placeholder {
border: 1px dashed #999;
padding: 20px;
text-align: center;
color: #666;
margin: 10px 0;
}
</style>
</head>
<body>
<h1>CS180 Project 2: Fun with Filters and Frequencies</h1>


<div class="section">
<h2>Part 1: Filters and Edges</h2>


<h3>1.1 Convolution</h3>
<div class="image-placeholder">Add images/results here</div>
<p> The runtime is nearly identical for greyscale images between my convolution and scipy's .convolve2d, although mine
  is slightly slower with color images. To deal with padding boundaries I initially cropped but realized I was losing information,
  so I ended up reflecting the boundaries like scipy did. I implemented my convolution with four for loops, and then two, saving time
  by multiplying patches of the image by the kernel instead of going element by element. I implemented padding with zeroes. I ran my two
  loop convolution against scipy's convolution on an image of me in grayscale using: a box filter, a Dx filter, and a Dy filter.
  Here is my code for this section:</p>
<pre><code class="language-python">def four_for_convolution(input_image, filter_type="box_filter", kernel_size=9):
    if filter_type == "box_filter":
        kernel_values = np.full((kernel_size, kernel_size), 1/(kernel_size*kernel_size))
        kernel_array = np.flip(kernel_values)
        pad_h = kernel_size // 2
        pad_w = kernel_size // 2
    if filter_type == "dx":
        kernel_values = np.array([[-1, 0, 1]])
        kernel_array = np.flip(kernel_values)
        pad_h = 1
        pad_w = 0
    if filter_type == "dy":
        kernel_values = np.array([[-1], [0], [1]])
        kernel_array = np.flip(kernel_values)
        pad_h = 0
        pad_w = 1
    input_width = input_image.shape[1]
    input_height = input_image.shape[0]
    output_image = np.empty((input_height, input_width))

    # Pad the image with zeros
    padded_image = np.pad(input_image, ((pad_h, pad_h), (pad_w, pad_w)), mode='constant', constant_values=0)

    for y in np.arange(input_height):
        for x in np.arange(input_width):
            current_pixel = input_image[y,x] # 0,0 is top left, max,max is bottom right
            #now do the kernel convolution
            sum = 0
            for j in np.arange(kernel_array.shape[1]):
                for i in np.arange(kernel_array.shape[0]):
                        # checks the bounds of x and y of current pixel
                        hy = y + j
                        hx = x + i
                        sum += kernel_array[j, i] * padded_image[hy, hx]
            output_image[y,x] = sum
    return output_image

#two four loops convolution
def two_for_convolution(input_image, filter_type="box_filter", kernel_size=9):
    if filter_type == "box_filter":
        kernel_values = np.full((kernel_size, kernel_size), 1/(kernel_size*kernel_size))
        kernel_array = np.flip(kernel_values)
        pad_h = kernel_size // 2
        pad_w = kernel_size // 2
    if filter_type == "dx":
        kernel_array = np.array([[-1, 0, 1]])
        pad_h = 0
        pad_w = 1
    if filter_type == "dy":
        kernel_array = np.array([[-1], [0], [1]])
        pad_h = 1
        pad_w = 0
    input_width = input_image.shape[1]
    input_height = input_image.shape[0]
    output_image = np.empty((input_height, input_width))

    # Pad the image with zeros
    padded_image = np.pad(input_image, ((pad_h, pad_h), (pad_w, pad_w)), mode='constant', constant_values=0)

    for y in np.arange(input_height):
        for x in np.arange(input_width):
            #remember this is y and x of the padded image not the input image
            #it takes a subsection with padded zeroes if out of bounds
            image_subsection = padded_image[y:y + kernel_array.shape[0], x:x+kernel_array.shape[1]]
            # * is element wise multiplication @ is the matrix multiplcation
            # just need to check out of bounds
            #multiply arrays not pixels for each iteration of the loop
            output_image[y][x] = np.sum(image_subsection * kernel_array)
    output_image = crop_convolution(output_image, pad_h, pad_w)
    return output_image

#convolution calling scipy
def scipy_convolution(input_image, filter_type="box_filter", kernel_size=9, sigma=5):
    if filter_type == "box_filter":
        kernel_array = np.flip(np.full((kernel_size, kernel_size), 1/(kernel_size*kernel_size)))
    if filter_type == "dx":
        kernel_array = np.flip(np.array([[-1, 0, 1]]))
    if filter_type == "dy":
        kernel_array = np.flip(np.array([[-1], [0], [1]]))
    if filter_type == "gaussian":
        g_1d = cv2.getGaussianKernel((6*sigma) + 1, sigma)
        g_2d = np.outer(g_1d, g_1d)
        kernel_array = np.flip(g_2d)
    if filter_type == "DoG x":
        kernel_array = np.flip(DoG_filter("x"))
    if filter_type == "DoG y":
        kernel_array = np.flip(DoG_filter("y"))
    return scipy.signal.convolve2d(input_image, kernel_array, mode='same', boundary='symm')
</code></pre>

<h3>1.2 Partial Derivatives and Edges</h3>
<div class="image-placeholder">Add images/results here</div>
<p> I used the scipy.convolve2d function for all further parts. I calculated the partial derivatives in x and y
    for the cameraman image vy convolving the image with finite difference operators D_x and D_y. I calculated a gradient magnitude image by taking the Dx image and Dy image and calculating
  the sqrt of square(Dx) + square(Dy). This only displays the edges of the images, both images on the x plane and the y plane. However,
  without binarizing the edges, the gradient magnitude image looks very noisy To fix this, we suppress the noise while retaining edges
  by setting all pixels to either a 0 (black) or 1 (white) depending on if the pixel's grayscale value is larger than a certain threshold. For 
  my threshold I choose 60 as it retained most of the cameraman's detail while reducing the visible noise on the ground. Although I did
  find threshold 20 to be more aesthetically pleasing with the snow-like noise texture, even if it did have too much detail for the cameraman. 
</p>
<pre><code>
  def gradient_magnitude_image(input_image):
    dx = scipy_convolution(input_image, filter_type="dx")
    dy = scipy_convolution(input_image, filter_type="dy")
    return np.sqrt(np.square(dx) + np.square(dy))

  def binarize_gradient_magnitude(input_image, threshold=60):
      input_image = (input_image * 255).astype(np.uint8)
      return (input_image > threshold)
</code></pre>
  
<h3>1.3 Gaussian & DoG Filters</h3>
<div class="image-placeholder">Add images/results here</div>
<p>Even so, with just the difference operators Dx and Dy, the image contained a lot of noise that even binarizing the image
couldn't fix. To fix this I blurred the image using a gaussian kernel, and then repeated the procedure in the previous part. 
This got rid of almost all noise and made edges thicker and seemingly brighter. To make this faster I implemented DoG or
derivate of gaussian filters which looks the same but convolves the gaussian kernel with D_x and D_y before convolving with the image, 
which saves one convolution in total.</p>
<pre><code>
  def DoG_filter(type, sigma=2):
    g_size = 6 * sigma + 1
    g_1d = cv2.getGaussianKernel(g_size, sigma)
    g_2d = np.outer(g_1d, g_1d)
    input_image = np.flip(g_2d)
    if type == "x":
        DoG = scipy_convolution(input_image, "dx")
    if type == "y":
        DoG = scipy_convolution(input_image, "dy")
    return DoG

  def gradient_magnitude_image_DoG(input_image):
      dxog = scipy_convolution(input_image, filter_type="DoG x")
      dyog = scipy_convolution(input_image, filter_type="DoG y")
      return np.sqrt(np.square(dxog) + np.square(dyog))
</code></pre>

<div class="section">
<h2>Part 2: Applications</h2>


<h3>2.1 Unsharp Mask</h3>
<div class="image-placeholder">Add images/results here</div>
<p> I took a blurry image and sharpened it by subtracting low filters from my image to retain
  only the high frequencies. This makes the image appear sharper because it has stronger high frequencies. I did so
by running a gaussian filter on my image, which is a low pass filter. This means it only retains low frequencies. Subtracting
this gaussian filtered image from the original image retains only the high frequencies / edges. I then added these edges back
to the image to sharpen it. I also tried this on a image that was sharpened, blurred, and then sharpened again. However, this
does not look the same it only retained the high frequencies of the blurred image not the high frequencies of the original image, since
those were lost when I low pass / gaussian filter it and removed high frequencies</p>
<pre><code>
  def sharpen(input_image, alpha=1.0):
    blurred = scipy_convolution_color(input_image, "gaussian", 9, 5)
    mask = input_image - blurred
    # need to clip because this will make it stray or wrap from 255
    return np.clip(input_image + alpha * mask, 0, 255)

def blur_then_sharpen(input_image, alpha=1.0):
    first_sharpened_image = sharpen(input_image)
    blur = scipy_convolution_color(first_sharpened_image, "gaussian", 9, 5)
    return sharpen(blur)
</code></pre>

<h3>2.2 Hybrid Images</h3>
<div class="image-placeholder">Add images/results here</div>
<p>I created a hybrid image (a static image that changes in interpretation as a function of viewing distance) by overlaying
a high pass of one image (just the high frequencies) with a low pass of another image (just the low frequencies). From up
close the high frequencies dominate, so we mainly see the high frequency image but at a distance the low frequencies are all 
we can see. I used some alignment code to align all my images. I began with the images of Derek and his cat Nutmeg. I then did this
with two of my own images, one of me and my cat, and another of my cat and lebron james. I then plotted the fourier transform
of my low pass and high pass images compared to my original images to show which frequencies were being filtered out.</p>

<h3>2.3 + 2.4 Image Blending (Oraple)</h3>
<div class="image-placeholder">Add images/results here</div>
<p></p>

</body>
</html>






